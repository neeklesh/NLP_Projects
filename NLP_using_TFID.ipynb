{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPidwpZp/UlpUWBBiT0LYEE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neeklesh/NLP_Projects/blob/main/NLP_using_TFID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "MXBEEwFFdUsK",
        "outputId": "e1de854b-58ae-4c73-92bf-d123d178f274"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b33d63ebcf92>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    'Steps to follow\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
          ]
        }
      ],
      "source": [
        "'Steps to follow\n",
        "1)  import nltk library and dowload required libraries such as nltk(punkt), nlt(stopwords)\n",
        "2) load the required data into system.\n",
        "3) Convert the paragraph in to sentences using 'Tokenize' i.e sent_tokenize. Convert into words using word_tokenize\n",
        "4)clean the data using 'RE' library to clean the data\n",
        "5) Import PorterStemmer and WordNetLemmatizer. and then assign them using ps and ls.\n",
        "6)using for loop we can then remove the unwanted punctuation marks and only keep the a-zand A-Z words in the paragraph.\n",
        "  for this we need range of 0 - sentences.\n",
        "  using re substitue the everything which is not equal to '^' a-zA-Z in the paragraph with space.\n",
        "  then we lower all the sentences so that everything is in same format\n",
        "  lemmatize all the words that are not in the set of english stopwords\n",
        "  again join all the words\n",
        "  then append the words to new list corpus\n",
        "7)using Sklearn.feature_extraction.text import TfidfVectorizer. Assign it to a CV.\n",
        "8)lastly convert the matrix into an array using fit_transform().toarray(). Create bag of words using tfid\n",
        "9)We can then split the data into training and testing\n",
        "10) using various predictive analytics technique do the requied.'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "_pVV_Jf1rLwM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBoKLMHKriJB",
        "outputId": "4eb0211e-7df6-4856-dfa6-38a566eec216"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Humans communicate through some form of language either by text or speech. Now to make interactions between computers and humans, computers need to understand natural languages used by humans. Natural language processing is all about making computers learn, process, and manipulate natural languages\"\"\""
      ],
      "metadata": {
        "id": "Y6s-HxFKrujL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aKRVCvpxsQks",
        "outputId": "07eff604-646f-4643-df92-287d80bcf4b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Humans communicate through some form of language either by text or speech. Now to make interactions between computers and humans, computers need to understand natural languages used by humans. Natural language processing is all about making computers learn, process, and manipulate natural languages'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "3pqX2yBZsY62"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "6ENc_i3ss0g4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the data\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem  import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "0aJJpmMatemD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "corpus = []"
      ],
      "metadata": {
        "id": "4wUbF1vAtgBp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now we clean the data and change its font\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]',' ', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [ls.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "aJRTpP9Lul8S"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFNk7nfBvckE",
        "outputId": "2cfa08c1-d06f-4223-adfe-e96ac94d201e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['human communicate form language either text speech',\n",
              " 'make interaction computer human computer need understand natural language used human',\n",
              " 'natural language processing making computer learn process manipulate natural language']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "TF = TfidfVectorizer()\n",
        "X = TF.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "gd6mZ_0tvdzY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g70tERhkyFx2",
        "outputId": "c8f3e790-78fe-47e7-dd43-9224435d321f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.41074684, 0.        , 0.41074684, 0.41074684, 0.31238356,\n",
              "        0.        , 0.2425937 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.41074684, 0.41074684, 0.        , 0.        ],\n",
              "       [0.        , 0.46819492, 0.        , 0.        , 0.46819492,\n",
              "        0.30781003, 0.18179756, 0.        , 0.30781003, 0.        ,\n",
              "        0.        , 0.23409746, 0.30781003, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30781003, 0.30781003],\n",
              "       [0.        , 0.24955659, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.38760591, 0.32813692, 0.        , 0.32813692,\n",
              "        0.32813692, 0.49911318, 0.        , 0.32813692, 0.32813692,\n",
              "        0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwVjSsUsyLvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}